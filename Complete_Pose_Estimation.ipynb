{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1vSfCPzfSN8ZD3wFhA9VokA222puKmrgf",
      "authorship_tag": "ABX9TyOvgc+wIReU8AlrNKzdoVxG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aimlrl/Human-Pose-Estimation-MS-COCO/blob/master/Complete_Pose_Estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The problem of pose estimation is to detect the pose of the person in the image. Here if you see the following images: \n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1I4s10X1xXNEQCelZnJqurfACmvvI8FO_'>"
      ],
      "metadata": {
        "id": "6XCEIMPlsECn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Then you can clearly tell the pose of the person in each image. Well the question arrises is that how we were able to clearly tell the pose of the person. \n",
        "\n",
        "# Well, if you notice clearly in the above images then the pose of the person is determined by the positions of the joints in the body visibl in the images. Now, the question arrises is that overall how many joints are there in the body of the person. The answer is that there are overall 17 joints in the body of the person as shown in the image below on the left: \n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1sEvIgYdzovWcQ5gvYJdriaxkztDHpRGh'>"
      ],
      "metadata": {
        "id": "l2v_lQscynaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# As can be observed that there are 17 points on the image of the human body and majority of them are location of joints in the body. All of these 17 (in the image 18 are shown but 17 points is the standard in Human Pose Esimation and we take 17) points are called Keypoints. Out of the 17 keypoints, maximum keypoints are the joints in the body (Keypoint Indices 2, 1, 5, 3, 6, 4, 8, 11, 7, 9, 12, 10, 13) and the remaining keypoints (Keypoint indices 16, 14, 15, 17) indicate the position of Right Ear, Right Eye, Left Eye and Left Ear. \n",
        "\n",
        "# Change in the location of keypoints in the image marks the change in the posture or pose of the human body in the image. Therefore, we can say that the problem of Human Pose Estimation can be solved by detecting the location of all these 17 keypoints if possible (will explain why we  mentioned \"if possible\") to roughly (will explain why we mentioned \"roughly\") pixel location (at which the the keypoint is present) level accuracy. \n",
        "\n",
        "# Now, why we mentioned \"if possible\". Well we mentioned \"if possible\" because in some scenarios, the photographs of the people are clicked in such a manner that some keypoints are not present in the photograph as shown below: \n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1WFTITXOtnW3amVh1patDRJQf-GXFEXkz'>\n",
        "\n",
        "# In the picture above, it can be seen that not all the keypoints are present for the person in red T-shirt (Please note that this image has been taken just for reference to show you and explain you the concept, we are here only performing single person pose estimation). So, something like that can also happen with us in some of the images and hence therefore it won't be possible for us to detect all the 17 keypoints. \n",
        "\n",
        "# In other cases, it may happen that the image of a person is occluded by an object or it's been taken in dark region or with less light then we can still detect keypoints as shown in the image below: \n",
        "\n",
        "<img src = 'https://drive.google.com/uc?id=1vmS-bVNZ97U2VDqxT0CWYoOBg9C_UhDX'>\n",
        "\n"
      ],
      "metadata": {
        "id": "mVu49-Fq0j3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So, now the question arrises is that what will be the training data for this task ? Well the training data for this task is going to be our input image of a single person which we are going to insert it into our CNN and the labels will be the location of all the keypoints present in the image. Now, the question arrises is that how we can define the location of keypoints. The location of keypoints can be defined by (x,y) coordinates. But, as it can be the case that some keypoints are not even present in the image or are present but occluded by an object or in the region where the light is less. Therefore, we will be having a validity scalar value (validity = 0 for a keypoint if not present in the image, validity = 1 if occluded, validity = 2 if not occluded but present in the image having very less light) for each of the 17 keypoints present in the image. Therefore, every keypoint of all the 17 keypoints, there will be a 3d vector (x,y,validity) so for all the 17 keypoints, there will be a (17,3) matrix. \n",
        "\n",
        "#Therefore our training data will consist of Input Image of a single person and labels as a matrix of shape (17,3) and we will be training our network to output a prediction of shape (17,3) where this prediction will be \"roughly\" giving the location of keypoints at pixel location level accuracy. \n",
        "\n",
        "# Now, getting the predictions right to pixel location level accuracy is very difficult and it may also result in a lot of fluctuations in the loss function (exploding or vanishing gradients of the loss function), therefore, we will be little bit noise to the labels. "
      ],
      "metadata": {
        "id": "ohXP3CIU7Yr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In order to perform pose estimation on the images of a single person first, only the image of the person needs to be cropped out of the whole image. We should not train our network for performing pose estimation on the whole image and just on the cropped image of the person from inside the whole image (Now, think about it that why we wan to do something like that ?). So, to crop out an image of a person from the whole image, we need bounding box coordinates of the person in the image. In short, we have to detect person in the image and that can be done by an Object Detection Algorithm which will now be acting as a person detector. Therefore, before performing Human Pose Estimation, we have to forst perform Object Detection on the image and then crop out the image of a person from the whole image and warp it and then input it into the human pose estimation neural network to output a prediction of (17,3). Therefore, the flow of Human Pose Estimation will be: \n",
        "\n",
        "# Whole Image --> Object detection Neural Network --> Cropped Image of the person from the whole image --> image warping --> Human Pose Estimation Neural Network --> (17,3) prediction. \n",
        "\n",
        "# So, usually we have to train or use a pretrained object detector to detect person in the image through the bounding box and crop out the person image, warp it to some fixed size and then train Huamn Pose Estimation network through these warped images with labels as ground truth keypoint coordinates in the form of (17,3) matrix. \n",
        "\n",
        "# But the dataset which we are using to train our Human Pose Estimation Neural Network is MS COCO (Common Objects in Context) 20 GB dataset. This dataset is already annotated with the ground truth bounding boxes of different objects in the image (useful for training object detection) and 17 ground truth keypoints in the image of a person. \n",
        "\n",
        "# But it is our luck you can say that our Object Detector is already trained on MS COCO dataset, therefore we dont need to first train our object detector in this case and we can straightly crop out person images from the whole images using ground truth bounding boxes of persons present in our dataset and warp them and pass them to our created Human Pose Estimation Neural Network to output keypoint predictions. For object detection, you can use Tensorflow 2 Object Detection API. "
      ],
      "metadata": {
        "id": "0dUV1oDfCw17"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcdEFX4e5p2y"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow.compat.v1 as tf\n",
        "from PIL import Image\n",
        "from collections import namedtuple\n",
        "import glob\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_eTIHzK7wbe"
      },
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, lets first download the dataset"
      ],
      "metadata": {
        "id": "USs1w-qLINSF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Z_O9gO6VKz"
      },
      "source": [
        "! wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-x4_jASEX9a"
      },
      "source": [
        "! unzip /content/drive/MyDrive/annotations_trainval2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdqy6RLW5YV7"
      },
      "source": [
        "import numpy as np\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkp6G8QOhkY9"
      },
      "source": [
        "def compile_examples(json_annotations):\n",
        "\n",
        "  file_handle = open(json_annotations)\n",
        "  annotations = json.load(file_handle)\n",
        "\n",
        "  image_urls = list()\n",
        "\n",
        "  for image_detail in annotations[\"images\"]:\n",
        "    image_urls.append(image_detail[\"coco_url\"])\n",
        "\n",
        "  image_ids = list()\n",
        "\n",
        "  for single_annotation in annotations[\"annotations\"]:\n",
        "    image_ids.append(single_annotation[\"image_id\"])\n",
        "\n",
        "  urls_dict = dict()\n",
        "\n",
        "  for img_url in image_urls:\n",
        "    urls_dict[img_url.split(\"/\")[-1]] = img_url\n",
        "\n",
        "  image_details = dict()\n",
        "  images_added = list()\n",
        "\n",
        "  for single_annotation in annotations[\"annotations\"]:\n",
        "\n",
        "    img_filename = \"000000\"+str(single_annotation[\"image_id\"])+\".jpg\"\n",
        "\n",
        "    keypoints = np.array(single_annotation[\"keypoints\"]).reshape(17,3)\n",
        "    gt_validity = keypoints[:,2] > 0\n",
        "\n",
        "    if img_filename in list(urls_dict.keys()) and sum(gt_validity) > 0 \\\n",
        "    and single_annotation[\"iscrowd\"] == 0 and single_annotation[\"bbox\"][2] > 48 and single_annotation[\"bbox\"][3] > 64:\n",
        "\n",
        "      images_added.append(img_filename)\n",
        "      image_details[img_filename] = dict()\n",
        "      image_details[img_filename][\"url\"] = urls_dict[img_filename]\n",
        "      image_details[img_filename][\"num_keypoints\"] = single_annotation[\"num_keypoints\"]\n",
        "      image_details[img_filename][\"bbox\"] = single_annotation[\"bbox\"]\n",
        "      image_details[img_filename][\"keypoints\"] = single_annotation[\"keypoints\"]\n",
        "\n",
        "  return image_details"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, try to understand that what is going on the function above by dry running it and if possible, try to make it run parallely using concurrent.futures or multiprocessing library. "
      ],
      "metadata": {
        "id": "iwnbzWWdIVnP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1StiZdxlfeHj"
      },
      "source": [
        "training_annotations = compile_examples(\"/content/drive/MyDrive/annotations/person_keypoints_train2017.json\")\n",
        "\n",
        "json_string = json.dumps(training_annotations)\n",
        "json_file_handle = open(\"training_annotations.json\",\"w\")\n",
        "json_file_handle.write(json_string)\n",
        "json_file_handle.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdxlerCkdpFk"
      },
      "source": [
        "cv_annotations = compile_examples(\"/content/drive/MyDrive/annotations/person_keypoints_val2017.json\")\n",
        "\n",
        "json_string = json.dumps(cv_annotations)\n",
        "json_file_handle = open(\"cv_annotations.json\",\"w\")\n",
        "json_file_handle.write(json_string)\n",
        "json_file_handle.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM4h89eLEn19"
      },
      "source": [
        "import requests\n",
        "import time\n",
        "from requests.exceptions import ConnectionError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Snn4RGcPCdj5"
      },
      "source": [
        "! sudo python3 -m pip install \"requests[security]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW2NnVKCCiwU"
      },
      "source": [
        "! pip install pyopenssl ndg-httpsclient pyasn1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPwoapZuLcsW"
      },
      "source": [
        "def download_image(image_filename):\n",
        "\n",
        "  tries = 0\n",
        "  while True:\n",
        "    tries = tries + 1\n",
        "    try:\n",
        "      r = requests.get(training_annotations[image_filename][\"url\"])\n",
        "      file_handle = open(os.path.join(\"/content/drive/MyDrive/person_train_images\",image_filename),\"wb\")\n",
        "      file_handle.write(r.content)\n",
        "      file_handle.close()\n",
        "      print(\"Written {}\".format(image_filename))\n",
        "      break\n",
        "    except ConnectionError as err:\n",
        "      if tries == 11:\n",
        "        raise err\n",
        "      else:\n",
        "        time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn1tafdegYjZ"
      },
      "source": [
        "def download_cv_image(image_filename):\n",
        "\n",
        "  tries = 0\n",
        "  while True:\n",
        "    tries = tries + 1\n",
        "    try:\n",
        "      r = requests.get(cv_annotations[image_filename][\"url\"])\n",
        "      file_handle = open(os.path.join(\"/content/drive/MyDrive/person_cv_train_images\",image_filename),\"wb\")\n",
        "      file_handle.write(r.content)\n",
        "      file_handle.close()\n",
        "      print(\"Written {}\".format(image_filename))\n",
        "      break\n",
        "    except ConnectionError as err:\n",
        "      if tries == 11:\n",
        "        raise err\n",
        "      else:\n",
        "        time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1OFYhgoR1ne"
      },
      "source": [
        "os.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm30eMMbQ5su"
      },
      "source": [
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3v14RZUS1ZR"
      },
      "source": [
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  executor.map(download_image,training_annotations.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmQvHOgDAU7m"
      },
      "source": [
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "  executor.map(download_cv_image,cv_annotations.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALDLNGRVDfA0"
      },
      "source": [
        "len(os.listdir(\"/content/drive/MyDrive/person_train_images\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8KCvFEdD91k"
      },
      "source": [
        "len(os.listdir(\"/content/drive/MyDrive/person_cv_train_images\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDtxpNbr5vMk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import UnidentifiedImageError\n",
        "from scipy.ndimage import gaussian_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to complete the code above. Let's give you a hint, we are actually trying to generate the ground truth labels (keypoints) but not in the form of a matrix of shape (17,3) but in 3D matrix of shape:\n",
        "\n",
        "# (1/4*Width of the warped cropped person image, 1/4*Height of the warped cropped person image, 17)\n",
        "\n",
        "# Therefore, it can be said that the ground truth matrix of (17,3) is actually been converted into an image whose dimensions will be equal to the dimensions of the input warped image with 17 channels, called ground truth heatmap. Each channel of the ground truth heatmap will ba having all the pixels with zero pixel intensity except at the location of one of a ground truth keypoint location. Furthermore, to add little bit noise to the pixel intensity at the location of one of the keypoints at it's respective channel of the heatmap, all the channels are convolved with gaussian filter.\n",
        "\n",
        "# Now, why the ground truth label is called heatmap. It is called heatmap because there are 17 channels of the ground truth heatmap with each channel corresponding to one of the ground truth keypoints and the region in that specific channel of a ground truth keypoint is brighten at the location of that specific keypoint in the image. \n",
        "\n",
        "# To know more about gaussian filter, you can navigate to this link: \n",
        "# https://docs.opencv.org/4.x/d4/d13/tutorial_py_filtering.html\n",
        "\n",
        "#So, the size of the input warped image will be (256,192,3). This size has been actually taken from the following research paper: \n",
        "# https://arxiv.org/abs/1804.06208\n",
        "\n",
        "# Therefore, the gound truth label matrix of keypoints of shape (17,3) will be converted into ground truth heatmap of shape (1/4*256, 1/4*192, 17) = (64,48,17). We are converting the output into a smaller but not the same size as input warped image size becauase we don't want to increase the computational complexity of the training and loss function computation and it is also recommended in the above paper. "
      ],
      "metadata": {
        "id": "kXaCCbmwNlPK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw5jetV096vV"
      },
      "source": [
        "def create_gt_heatmap_labels(img,annotations,resize_shape=(64,48)):\n",
        "\n",
        "  gt_heatmap = np.zeros(shape=(17,resize_shape[0],resize_shape[1]))\n",
        "  keypoints = np.array(annotations[img][\"keypoints\"]).reshape(17,3)\n",
        "  xmin,ymin,w,h = annotations[img][\"bbox\"]\n",
        "  bbox_offset = np.array([xmin,ymin,0])\n",
        "  bbox_dims = np.array([w,h,1])\n",
        "  gt_heatmap_dims = np.array([resize_shape[1],resize_shape[0],1])\n",
        "  keypoints = np.round((keypoints - bbox_offset)*gt_heatmap_dims/bbox_dims).astype(int)\n",
        "\n",
        "  for i in range(17):\n",
        "\n",
        "    if keypoints[i,2] > 0:\n",
        "      y = keypoints[i,0]\n",
        "      x = keypoints[i,1]\n",
        "\n",
        "      if x < 0 or y < 0 or x >= resize_shape[0] or y >= resize_shape[1]:\n",
        "        keypoints[i,2] = 0.0\n",
        "        continue\n",
        "\n",
        "      gt_heatmap[i,x,y] = 1.0\n",
        "      gt_heatmap[i,:,:] = gaussian_filter(input=gt_heatmap[i,:,:],sigma=2,mode=\"constant\",cval=0.0)\n",
        "      gt_heatmap[i,:,:] = gt_heatmap[i,:,:]/np.max(gt_heatmap[i,:,:])\n",
        "\n",
        "  gt_validity = keypoints[:,2] > 0\n",
        "  gt_validity = gt_validity.reshape(gt_validity.shape[0],1,1)\n",
        "\n",
        "  return gt_heatmap.astype(float), gt_validity.astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The below function is the custom data generator for generating the batches of training examples. Every single batch will be having 32 warped cropped images of persons that is the size of each batch of input images will be (32,256,192,3) and also the respective heatmaps of each of the warped cropped images in the batch, that is (32,64,48,17). This custom data generator is going to get the ground truth heatmaps from the function above by calling it. "
      ],
      "metadata": {
        "id": "p6BatKtXRR5W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slpo4QetYEVe"
      },
      "source": [
        "def train_generator(train_images,annotations,batch_size=32,resize_shape=(256,192)):\n",
        "\n",
        "  num_train_images = len(train_images)\n",
        "  train_images = np.array(train_images)\n",
        "  img_norm_mean = np.array([0.485,0.456,0.456])\n",
        "  img_norm_std = np.array([0.229,0.224,0.225])\n",
        "\n",
        "  while True:\n",
        "\n",
        "    for offset in range(0,num_train_images,batch_size):\n",
        "      \n",
        "      batch_images = list(np.random.choice(train_images,size=batch_size,replace=False))\n",
        "      images_batch = []\n",
        "      gt_heatmaps_batch = []\n",
        "      gt_validities_batch = []\n",
        "\n",
        "      for img in batch_images:\n",
        "\n",
        "        try:\n",
        "          image = Image.open(os.path.join(\"/content/drive/MyDrive/person_train_images\",img))\n",
        "          xmin,ymin,w,h = annotations[img][\"bbox\"]\n",
        "          cropped_image = image.resize(size=(resize_shape[1],resize_shape[0]),box=(xmin,ymin,xmin+w,ymin+h))\n",
        "          cropped_image = np.array(cropped_image)\n",
        "\n",
        "          if len(cropped_image.shape) != 3:\n",
        "            cropped_image = np.stack((cropped_image,)*3,axis=-1)\n",
        "\n",
        "          cropped_image = cropped_image.astype(float)/255.0\n",
        "          cropped_image = (cropped_image - img_norm_mean)/img_norm_std\n",
        "\n",
        "          cropped_image = cropped_image.reshape(resize_shape[0],resize_shape[1],3)\n",
        "          images_batch.append(cropped_image)\n",
        "\n",
        "          gt_heatmap,gt_validity = create_gt_heatmap_labels(img,annotations)\n",
        "          gt_heatmaps_batch.append(gt_heatmap)\n",
        "          gt_validities_batch.append(gt_validity)\n",
        "\n",
        "        except UnidentifiedImageError as err:\n",
        "          continue\n",
        "\n",
        "      images_batch = np.array(images_batch)\n",
        "      gt_heatmaps_batch = np.array(gt_heatmaps_batch)\n",
        "      gt_validities_batch = np.array(gt_validities_batch)\n",
        "\n",
        "      yield images_batch,gt_heatmaps_batch,gt_validities_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, it's upto you to implement the Cross Validation data generator"
      ],
      "metadata": {
        "id": "6K6GLURlTMa4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-bMVUBAlEox"
      },
      "source": [
        "def cv_generator(cv_images,annotations,batch_size=32,resize_shape=(256,192)):\n",
        "\n",
        "  num_cv_train_images = len(cv_images)\n",
        "  img_norm_mean = np.array([0.485,0.456,0.456])\n",
        "  img_norm_std = np.array([0.229,0.224,0.225])\n",
        "\n",
        "  while True:\n",
        "\n",
        "    for offset in range(0,num_cv_train_images,batch_size):\n",
        "      \n",
        "      batch_images = cv_images[offset:offset+batch_size]\n",
        "      images_batch = []\n",
        "      gt_heatmaps_batch = []\n",
        "      gt_validities_batch = []\n",
        "\n",
        "      for img in batch_images:\n",
        "\n",
        "        try:\n",
        "          image = Image.open(os.path.join(\"/content/drive/MyDrive/person_cv_train_images\",img))\n",
        "          xmin,ymin,w,h = annotations[img][\"bbox\"]\n",
        "          cropped_image = image.resize(size=(resize_shape[1],resize_shape[0]),box=(xmin,ymin,xmin+w,ymin+h))\n",
        "          cropped_image = np.array(cropped_image)\n",
        "\n",
        "          if len(cropped_image.shape) != 3:\n",
        "            cropped_image = np.stack((cropped_image,)*3,axis=-1)\n",
        "\n",
        "          cropped_image = cropped_image.astype(float)/255.0\n",
        "          cropped_image = (cropped_image - img_norm_mean)/img_norm_std\n",
        "\n",
        "          cropped_image = cropped_image.reshape(resize_shape[0],resize_shape[1],3)\n",
        "          images_batch.append(cropped_image)\n",
        "\n",
        "          gt_heatmap,gt_validity = create_gt_heatmap_labels(img,annotations)\n",
        "          gt_heatmaps_batch.append(gt_heatmap)\n",
        "          gt_validities_batch.append(gt_validity)\n",
        "\n",
        "        except UnidentifiedImageError as err:\n",
        "          continue\n",
        "\n",
        "      images_batch = np.array(images_batch)\n",
        "      gt_heatmaps_batch = np.array(gt_heatmaps_batch)\n",
        "      gt_validities_batch = np.array(gt_validities_batch)\n",
        "\n",
        "      yield images_batch,gt_heatmaps_batch,gt_validities_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzT3W7URI78P"
      },
      "source": [
        "from keras.layers import Conv2D,Conv2DTranspose,BatchNormalization\n",
        "from keras.applications import resnet_v2\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import ReLU\n",
        "from keras.layers import Reshape\n",
        "from keras.initializers import random_normal\n",
        "import tensorflow as tf\n",
        "import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the above research paper and implement the Human Pose Estimation Neural Network using Keras:\n",
        "# https://arxiv.org/abs/1804.06208\n",
        "\n",
        "# and fill up the following function. "
      ],
      "metadata": {
        "id": "3m4uXdk1QvaE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQdoh2ZsDFun"
      },
      "source": [
        "def create_pretrained_pose_resnet(resize_shape=(64,48)):\n",
        "\n",
        "  images_batch = Input(shape=(256,192,3))\n",
        "  pretrained_resnet = resnet_v2.ResNet50V2(include_top=False,input_shape=(256,192,3))\n",
        "\n",
        "  pretrained_resnet.trainable = False\n",
        "\n",
        "  pretrained_resnet_output = pretrained_resnet(images_batch,training=False)\n",
        "\n",
        "  pretrained_resnet_out = Conv2DTranspose(filters=256,kernel_size=4,strides=2,padding=\"same\",\n",
        "                                          kernel_initializer=random_normal(stddev=0.001))(pretrained_resnet_output)\n",
        "  pretrained_resnet_out = BatchNormalization()(pretrained_resnet_out)\n",
        "  pretrained_resnet_out = ReLU()(pretrained_resnet_out)\n",
        "\n",
        "  pretrained_resnet_out = Conv2DTranspose(filters=256,kernel_size=4,strides=2,padding=\"same\",\n",
        "                                          kernel_initializer=random_normal(stddev=0.001))(pretrained_resnet_out)\n",
        "  pretrained_resnet_out = BatchNormalization()(pretrained_resnet_out)\n",
        "  pretrained_resnet_out = ReLU()(pretrained_resnet_out)\n",
        "\n",
        "  pretrained_resnet_out = Conv2DTranspose(filters=256,kernel_size=4,strides=2,padding=\"same\",\n",
        "                                          kernel_initializer=random_normal(stddev=0.001))(pretrained_resnet_out)\n",
        "  pretrained_resnet_out = BatchNormalization()(pretrained_resnet_out)\n",
        "  pretrained_resnet_out = ReLU()(pretrained_resnet_out)\n",
        "\n",
        "  pretrained_resnet_out = Conv2D(filters=17,kernel_size=1,kernel_initializer=random_normal(stddev=0.001))(pretrained_resnet_out)\n",
        "\n",
        "  pretrained_resnet_out = Reshape((17,resize_shape[0],resize_shape[1]))(pretrained_resnet_out)\n",
        "\n",
        "  pretrained_pose_resnet = Model(images_batch,pretrained_resnet_out)\n",
        "\n",
        "  return pretrained_pose_resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsAKBi_P9BF6"
      },
      "source": [
        "mse_loss_fn = tf.keras.losses.MeanSquaredError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwC5WK0Q9oBX"
      },
      "source": [
        "def mse_loss(heatmap_pred,heatmap_train,heatmap_val_train):\n",
        "\n",
        "  heatmap_pred = heatmap_val_train * tf.cast(heatmap_pred,tf.float64)\n",
        "  heatmap_train = heatmap_val_train * heatmap_train\n",
        "\n",
        "  mse = mse_loss_fn(y_true=heatmap_train,y_pred=tf.cast(heatmap_pred,tf.float64))\n",
        "  return mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCoDg8cLVKFM"
      },
      "source": [
        "pose_estimate_optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JEMuxEdx3ER"
      },
      "source": [
        "pose_estimate_finetune_optimizer = keras.optimizers.adam_v2.Adam(learning_rate=0.0000001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edPBgSSNaEMG"
      },
      "source": [
        "pretrained_pose_resnet = create_pretrained_pose_resnet()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTBwU1goz5tT"
      },
      "source": [
        "pretrained_pose_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mux-MQjFZV2c"
      },
      "source": [
        "pose_estimate_checkpoint_dir = \"/content/drive/MyDrive/pose_estimate_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(pose_estimate_checkpoint_dir,\"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=pose_estimate_optimizer,model=pretrained_pose_resnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVMw5dgJyR9f"
      },
      "source": [
        "pose_estimate_finetune_checkpoint_dir = \"/content/drive/MyDrive/pose_estimate_finetune_checkpoints\"\n",
        "finetune_checkpoint_prefix = os.path.join(pose_estimate_finetune_checkpoint_dir,\"ckpt\")\n",
        "finetune_checkpoint = tf.train.Checkpoint(optimizer=pose_estimate_finetune_optimizer,model=pretrained_pose_resnet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF8YGpnOaPzS"
      },
      "source": [
        "@tf.function\n",
        "def train_step(images_batch,heatmap_train_batch,heatmap_val_batch):\n",
        "\n",
        "    with tf.GradientTape() as pose_estimate_tape:\n",
        "\n",
        "      pretrained_pose_resnet.trainable = True\n",
        "      pretrained_pose_resnet.layers[1].trainable = False\n",
        "      \n",
        "      heatmap_pred_batch = pretrained_pose_resnet(images_batch,training=True)\n",
        "\n",
        "      pose_estimate_loss = mse_loss(heatmap_pred=heatmap_pred_batch,heatmap_train=heatmap_train_batch,heatmap_val_train=heatmap_val_batch)\n",
        "\n",
        "      pose_estimate_gradients = pose_estimate_tape.gradient(pose_estimate_loss, pretrained_pose_resnet.trainable_variables)\n",
        "      pose_estimate_optimizer.apply_gradients(zip(pose_estimate_gradients, pretrained_pose_resnet.trainable_variables))\n",
        "\n",
        "    return heatmap_pred_batch,pose_estimate_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dod_V1szv_V3"
      },
      "source": [
        "@tf.function\n",
        "def finetune_train_step(images_batch,heatmap_train_batch,heatmap_val_batch):\n",
        "\n",
        "  pretrained_pose_resnet.trainable = True\n",
        "\n",
        "  for layer in pretrained_pose_resnet.layers[1].layers[:-13]:\n",
        "    layer.trainable = False\n",
        "\n",
        "    with tf.GradientTape() as pose_estimate_tape:\n",
        "      \n",
        "      heatmap_pred_batch = pretrained_pose_resnet(images_batch,training=True)\n",
        "\n",
        "      for layer in pretrained_pose_resnet.layers[1].layers[177:]:\n",
        "        if \"bn\" in layer.name:\n",
        "          layer.trainable = False\n",
        "\n",
        "      pose_estimate_loss = mse_loss(heatmap_pred=heatmap_pred_batch,heatmap_train=heatmap_train_batch,heatmap_val_train=heatmap_val_batch)\n",
        "\n",
        "      pose_estimate_gradients = pose_estimate_tape.gradient(pose_estimate_loss, pretrained_pose_resnet.trainable_variables)\n",
        "      pose_estimate_finetune_optimizer.apply_gradients(zip(pose_estimate_gradients, pretrained_pose_resnet.trainable_variables))\n",
        "\n",
        "    return heatmap_pred_batch,pose_estimate_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl5u3NJqivVQ"
      },
      "source": [
        "@tf.function\n",
        "def cv_step(cv_images_batch,cv_heatmap_train_batch,cv_heatmap_val_batch):\n",
        "\n",
        "  pretrained_pose_resnet.trainable = False\n",
        "      \n",
        "  cv_heatmap_pred_batch = pretrained_pose_resnet(cv_images_batch,training=False)\n",
        "\n",
        "  pose_estimate_cv_loss = mse_loss(heatmap_pred=cv_heatmap_pred_batch,heatmap_train=cv_heatmap_train_batch,\n",
        "                                  heatmap_val_train=cv_heatmap_val_batch)\n",
        "\n",
        "  return cv_heatmap_pred_batch,pose_estimate_cv_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HhFraqrhqub"
      },
      "source": [
        "epochs = 90\n",
        "batch_size = 116\n",
        "cv_batch_size = 149"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYxmt-9gyG5Z"
      },
      "source": [
        "finetune_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tylu8Qww5u6R"
      },
      "source": [
        "# ORIGINAL FROM HERE:\n",
        "# https://github.com/microsoft/human-pose-estimation.pytorch/blob/715d29e55f59ae555116542e85ed7175d57120e6/lib/core/evaluate.py\n",
        "# ------------------------------------------------------------------------------\n",
        "# Copyright (c) Microsoft\n",
        "# Licensed under the MIT License.\n",
        "# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n",
        "# ------------------------------------------------------------------------------\n",
        "# Calculates Percentage of Correct Key-points (PCK) accuracy\n",
        "# A detected joint is considered correct if the distance between the predicted \n",
        "# and the true joint is within a certain threshold. \n",
        "\n",
        "\n",
        "def get_max_preds(batch_heatmaps):\n",
        "    '''\n",
        "    get predictions from score maps\n",
        "    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n",
        "    '''\n",
        "\n",
        "    assert isinstance(batch_heatmaps, np.ndarray), 'batch_heatmaps should be numpy.ndarray'\n",
        "    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n",
        "\n",
        "    batch_size = batch_heatmaps.shape[0]\n",
        "    num_joints = batch_heatmaps.shape[1]\n",
        "    width = batch_heatmaps.shape[3]\n",
        "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n",
        "    idx = np.argmax(heatmaps_reshaped, 2)\n",
        "    maxvals = np.amax(heatmaps_reshaped, 2)\n",
        "\n",
        "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n",
        "    idx = idx.reshape((batch_size, num_joints, 1))\n",
        "\n",
        "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
        "\n",
        "    preds[:, :, 0] = (preds[:, :, 0]) % width\n",
        "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n",
        "\n",
        "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n",
        "    pred_mask = pred_mask.astype(np.float32)\n",
        "\n",
        "    preds *= pred_mask\n",
        "    return preds, maxvals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pdOfoTM57v_"
      },
      "source": [
        "def calc_dists(preds, target, normalize):\n",
        "    preds = preds.astype(np.float32)\n",
        "    target = target.astype(np.float32)\n",
        "    dists = np.zeros((preds.shape[1], preds.shape[0]))\n",
        "    for n in range(preds.shape[0]):\n",
        "        for c in range(preds.shape[1]):\n",
        "            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n",
        "                normed_preds = preds[n, c, :] / normalize[n]\n",
        "                normed_targets = target[n, c, :] / normalize[n]\n",
        "                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n",
        "            else:\n",
        "                dists[c, n] = -1\n",
        "    return dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_K0wm-o6Aqq"
      },
      "source": [
        "def dist_acc(dists, thr=0.5):\n",
        "    ''' Return percentage below threshold while ignoring values with a -1 '''\n",
        "    dist_cal = np.not_equal(dists, -1)\n",
        "    num_dist_cal = dist_cal.sum()\n",
        "    if num_dist_cal > 0:\n",
        "        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i1j5E6Q6Em0"
      },
      "source": [
        "def accuracy(output, target, hm_type='gaussian', thr=0.5):\n",
        "    '''\n",
        "    Calculate accuracy according to PCK,\n",
        "    but uses ground truth heatmap rather than x,y locations\n",
        "    First value to be returned is average accuracy across 'idxs',\n",
        "    followed by individual accuracies\n",
        "    '''\n",
        "    idx = list(range(output.shape[1]))\n",
        "    norm = 1.0\n",
        "    if hm_type == 'gaussian':\n",
        "        pred, _ = get_max_preds(output.numpy())\n",
        "        target, _ = get_max_preds(target)\n",
        "        h = output.shape[2]\n",
        "        w = output.shape[3]\n",
        "        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n",
        "    dists = calc_dists(pred, target, norm)\n",
        "\n",
        "    acc = np.zeros((len(idx) + 1))\n",
        "    avg_acc = 0\n",
        "    cnt = 0\n",
        "\n",
        "    for i in range(len(idx)):\n",
        "        acc[i + 1] = dist_acc(dists[idx[i]],thr=thr)\n",
        "        if acc[i + 1] >= 0:\n",
        "            avg_acc = avg_acc + acc[i + 1]\n",
        "            cnt += 1\n",
        "\n",
        "    avg_acc = avg_acc / cnt if cnt != 0 else 0\n",
        "    if cnt != 0:\n",
        "        acc[0] = avg_acc\n",
        "    return avg_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The above four functions which you can see are taken from a github repository to implement the calculation of special kind of accuracy metric used for Human Pose Estimation Neural Network. This special kind of accuracy metric is called PCK Accuracy (Percentage of Correct Keypoints Accuracy). So, how it is evaluated ? Well, in short what we do is that for every prediction of the network of shape (64,48,17), we compute the difference between the location of predicted keypoints and location of ground truth keypoints for the 17 heatmaps and if the difference between the locations is less than 0.5 then we consider the predicted keypoint to be detected correctly and count it as correct keypoint detection and similarly we do it for all the other remaining 16 keypoints and then compute the fraction of correctly predicted keypoints out of 17 keypoints. \n",
        "\n",
        "# Similarly, we do it for all the images in a batch of 32 images and take the average of the fraction of correct keypoints over all the batch of images. And that is what has been done in the above four functions. "
      ],
      "metadata": {
        "id": "FAMoKk8jTu5S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfZKyNHD6H6u"
      },
      "source": [
        "class pck_accuracy_metric(keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self,name=\"pck_accuracy\",**kwargs):\n",
        "    super().__init__(name=name,**kwargs)\n",
        "    self.pck_avg_accuracy_sum = self.add_weight(name=\"pck_avg_accuracy_sum\",initializer=\"zeros\",dtype=\"float32\")\n",
        "    self.total_batches = self.add_weight(name=\"total_batches\",initializer=\"zeros\",dtype=\"int32\")\n",
        "\n",
        "  def update_state(self,y_true,y_pred,sample_weight=None):\n",
        "    pck_avg_accuracy = accuracy(output=y_pred,target=y_true,thr=0.5)\n",
        "    self.pck_avg_accuracy_sum.assign_add(pck_avg_accuracy)\n",
        "    self.total_batches.assign_add(1)\n",
        "\n",
        "  def result(self):\n",
        "    return self.pck_avg_accuracy_sum/tf.cast(self.total_batches,tf.float32)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.pck_avg_accuracy_sum.assign(0.0)\n",
        "    self.total_batches.assign(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yc9urvHPal2"
      },
      "source": [
        "def lr_scheduler(epoch,lr):\n",
        "    \n",
        "    lr = 1e-3\n",
        "\n",
        "    if epoch > 120:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 90:\n",
        "        lr *= 1e-1\n",
        "\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nB0a3StlTL-H"
      },
      "source": [
        "pose_train_acc_metric = pck_accuracy_metric()\n",
        "pose_val_acc_metric = pck_accuracy_metric()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbwG8txvLFFp"
      },
      "source": [
        "json_file_handle = open(\"/content/drive/MyDrive/training_annotations.json\")\n",
        "json_string = json_file_handle.read()\n",
        "training_annotations = json.loads(json_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iLxAFw043vu"
      },
      "source": [
        "json_file_handle = open(\"/content/drive/MyDrive/cv_annotations.json\")\n",
        "json_string = json_file_handle.read()\n",
        "cv_annotations = json.loads(json_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Please also write the training loop to train the network below. "
      ],
      "metadata": {
        "id": "o1ZZoj02VhW3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS6v3Xn3d70h"
      },
      "source": [
        "def train(epochs):\n",
        "\n",
        "  train_images = os.listdir(\"/content/drive/MyDrive/person_train_images\")\n",
        "  cv_images = os.listdir(\"/content/drive/MyDrive/person_cv_train_images\")\n",
        "\n",
        "  cv_datagen = cv_generator(cv_images,cv_annotations,batch_size=cv_batch_size)\n",
        "\n",
        "  #callback.on_train_begin()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    time_step = 1\n",
        "\n",
        "    #callback.on_epoch_begin(epoch=epoch)\n",
        "\n",
        "    for images_batch,gt_heatmaps_batch,gt_validities_batch in train_generator(train_images,\n",
        "                                                                              training_annotations,batch_size):\n",
        "\n",
        "      heatmaps_pred_batch,pose_estimate_loss = train_step(images_batch,gt_heatmaps_batch,gt_validities_batch)\n",
        "\n",
        "      if time_step > len(os.listdir(\"/content/drive/MyDrive/person_train_images\"))//batch_size:\n",
        "        break\n",
        "\n",
        "      if time_step % 100 == 0:\n",
        "\n",
        "        pose_train_acc_metric.update_state(gt_heatmaps_batch,heatmaps_pred_batch)\n",
        "        training_pck_accuracy = pose_train_acc_metric.result()\n",
        "\n",
        "        \"\"\"\n",
        "        cv_time_step = 1\n",
        "        avg_cv_loss = 0\n",
        "\n",
        "        for cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch in cv_generator(cv_images,cv_annotations,\n",
        "                                                                                        batch_size=cv_batch_size):\n",
        "          \n",
        "          cv_heatmaps_pred_batch,pose_estimate_cv_loss = cv_step(cv_images_batch,cv_gt_heatmaps_batch,\n",
        "                                                                 cv_gt_validities_batch)\n",
        "          \n",
        "          if cv_time_step > len(os.listdir(\"/content/drive/MyDrive/person_cv_train_images\"))//cv_batch_size:\n",
        "            break\n",
        "\n",
        "          pose_val_acc_metric.update_state(cv_gt_heatmaps_batch,cv_heatmaps_pred_batch)\n",
        "          avg_cv_loss = avg_cv_loss + float(pose_estimate_cv_loss)\n",
        "          cv_time_step = cv_time_step + 1\n",
        "\n",
        "        avg_cv_loss = avg_cv_loss/float(cv_time_step)\n",
        "        cv_pck_accuracy = pose_val_acc_metric.result()\n",
        "        \n",
        "        \"\"\"\n",
        "        cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch = next(cv_datagen)\n",
        "        cv_heatmaps_pred_batch,pose_estimate_cv_loss = cv_step(cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch)\n",
        "        pose_val_acc_metric.update_state(cv_gt_heatmaps_batch,cv_heatmaps_pred_batch)\n",
        "        cv_pck_accuracy = pose_val_acc_metric.result()\n",
        "\n",
        "        print(\"Epoch: {} Time Step: {} Training Loss: {} Training Accuracy: {} Val Loss: {} Val Accuracy: {}\".format(epoch,time_step,float(pose_estimate_loss),\n",
        "                                                                                                                     float(training_pck_accuracy),\n",
        "                                                                                                                     float(pose_estimate_cv_loss),\n",
        "                                                                                                                     float(cv_pck_accuracy)))\n",
        "        \n",
        "        pose_train_acc_metric.reset_state()\n",
        "        pose_val_acc_metric.reset_state()\n",
        "\n",
        "      #print(\"Epoch: {},Time Step: {}\".format(epoch,time_step))\n",
        "\n",
        "      time_step = time_step + 1\n",
        "\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcYfnnENxJ8o"
      },
      "source": [
        "def finetune_train(epochs):\n",
        "\n",
        "  train_images = os.listdir(\"/content/drive/MyDrive/person_train_images\")\n",
        "  cv_images = os.listdir(\"/content/drive/MyDrive/person_cv_train_images\")\n",
        "\n",
        "  cv_datagen = cv_generator(cv_images,cv_annotations,batch_size=cv_batch_size)\n",
        "\n",
        "  #callback.on_train_begin()\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    time_step = 1\n",
        "\n",
        "    #callback.on_epoch_begin(epoch=epoch)\n",
        "\n",
        "    for images_batch,gt_heatmaps_batch,gt_validities_batch in train_generator(train_images,\n",
        "                                                                              training_annotations,batch_size):\n",
        "\n",
        "      heatmaps_pred_batch,pose_estimate_loss = finetune_train_step(images_batch,gt_heatmaps_batch,gt_validities_batch)\n",
        "\n",
        "      if time_step > len(os.listdir(\"/content/drive/MyDrive/person_train_images\"))//batch_size:\n",
        "        break\n",
        "\n",
        "      if time_step % 100 == 0:\n",
        "\n",
        "        pose_train_acc_metric.update_state(gt_heatmaps_batch,heatmaps_pred_batch)\n",
        "        training_pck_accuracy = pose_train_acc_metric.result()\n",
        "\n",
        "        \"\"\"\n",
        "        cv_time_step = 1\n",
        "        avg_cv_loss = 0\n",
        "\n",
        "        for cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch in cv_generator(cv_images,cv_annotations,\n",
        "                                                                                        batch_size=cv_batch_size):\n",
        "          \n",
        "          cv_heatmaps_pred_batch,pose_estimate_cv_loss = cv_step(cv_images_batch,cv_gt_heatmaps_batch,\n",
        "                                                                 cv_gt_validities_batch)\n",
        "          \n",
        "          if cv_time_step > len(os.listdir(\"/content/drive/MyDrive/person_cv_train_images\"))//cv_batch_size:\n",
        "            break\n",
        "\n",
        "          pose_val_acc_metric.update_state(cv_gt_heatmaps_batch,cv_heatmaps_pred_batch)\n",
        "          avg_cv_loss = avg_cv_loss + float(pose_estimate_cv_loss)\n",
        "          cv_time_step = cv_time_step + 1\n",
        "\n",
        "        avg_cv_loss = avg_cv_loss/float(cv_time_step)\n",
        "        cv_pck_accuracy = pose_val_acc_metric.result()\n",
        "        \n",
        "        \"\"\"\n",
        "        cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch = next(cv_datagen)\n",
        "        cv_heatmaps_pred_batch,pose_estimate_cv_loss = cv_step(cv_images_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch)\n",
        "        pose_val_acc_metric.update_state(cv_gt_heatmaps_batch,cv_heatmaps_pred_batch)\n",
        "        cv_pck_accuracy = pose_val_acc_metric.result()\n",
        "\n",
        "        print(\"Epoch: {} Time Step: {} Training Loss: {} Training Accuracy: {} Val Loss: {} Val Accuracy: {}\".format(epoch,time_step,float(pose_estimate_loss),\n",
        "                                                                                                                     float(training_pck_accuracy),\n",
        "                                                                                                                     float(pose_estimate_cv_loss),\n",
        "                                                                                                                     float(cv_pck_accuracy)))\n",
        "        \n",
        "        pose_train_acc_metric.reset_state()\n",
        "        pose_val_acc_metric.reset_state()\n",
        "\n",
        "      #print(\"Epoch: {},Time Step: {}\".format(epoch,time_step))\n",
        "\n",
        "      time_step = time_step + 1\n",
        "\n",
        "    finetune_checkpoint.save(file_prefix = finetune_checkpoint_prefix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp-4vmj4i_w-"
      },
      "source": [
        "train(epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2dpaIm8pvjR"
      },
      "source": [
        "latest_checkpoint = tf.train.latest_checkpoint(\"/content/drive/MyDrive/pose_estimate_checkpoints\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnL0plIBi19C"
      },
      "source": [
        "latest_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oN5ybjGqhi8"
      },
      "source": [
        "finetune_train(finetune_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9A4taE8oYEZ"
      },
      "source": [
        "finetune_latest_checkpoint = tf.train.latest_checkpoint(\"/content/drive/MyDrive/pose_estimate_finetune_checkpoints\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrnC9AB5okmo"
      },
      "source": [
        "finetune_latest_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQzT3lhDrFt-"
      },
      "source": [
        "finetune_checkpoint.restore(finetune_latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E76aYwl_taIQ"
      },
      "source": [
        "cv_images = os.listdir(\"/content/drive/MyDrive/person_cv_train_images\")\n",
        "cv_datagen = cv_generator(cv_images,cv_annotations,batch_size=cv_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU7RQwZQsAjW"
      },
      "source": [
        "def plotting_data(test_img,heatmap,test_validity):\n",
        "\n",
        "  test_img = test_img[0]\n",
        "\n",
        "  mean=np.array([0.485, 0.456, 0.406])\n",
        "  std=np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "  test_img = test_img*std + mean\n",
        "  heatmap = np.sum(heatmap[0],axis=0)\n",
        "\n",
        "  fig = plt.figure(2,figsize=(20,20))\n",
        "  plt.gray()\n",
        "\n",
        "  ax1 = fig.add_subplot(121)\n",
        "  ax1.imshow(test_img)\n",
        "\n",
        "  ax2 = fig.add_subplot(122)\n",
        "  ax2.imshow(heatmap)\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpTsSy2puIHZ"
      },
      "source": [
        "cv_imgs_batch,cv_gt_heatmaps_batch,cv_gt_validities_batch = next(cv_datagen)\n",
        "single_cv_img = cv_imgs_batch[0,:,:,:]\n",
        "single_cv_img = single_cv_img.reshape(1,single_cv_img.shape[0],single_cv_img.shape[1],single_cv_img.shape[2])\n",
        "single_cv_gt_heatmap = cv_gt_heatmaps_batch[0,:,:,:]\n",
        "single_cv_gt_heatmap = single_cv_gt_heatmap.reshape(1,single_cv_gt_heatmap.shape[0],single_cv_gt_heatmap.shape[1],single_cv_gt_heatmap.shape[2])\n",
        "single_cv_gt_validity = cv_gt_validities_batch[0,:,:,:]\n",
        "single_cv_gt_validity = single_cv_gt_validity.reshape(1,single_cv_gt_validity.shape[0],single_cv_gt_validity.shape[1],single_cv_gt_validity.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7nHOGT-0jLv"
      },
      "source": [
        "single_cv_img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PERPFH-u0yU_"
      },
      "source": [
        "single_cv_gt_heatmap.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwulVfZW014h"
      },
      "source": [
        "single_cv_gt_validity.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rs_q2mnt0_XX"
      },
      "source": [
        "pretrained_pose_resnet.trainable = False\n",
        "cv_img_heatmap_pred = pretrained_pose_resnet(single_cv_img,training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRogne7e1G-L"
      },
      "source": [
        "cv_img_heatmap_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpTRkc-i185I"
      },
      "source": [
        "print(\"GROUNDTRUTH HEATMAP\")\n",
        "plotting_data(single_cv_img,single_cv_gt_heatmap,single_cv_gt_validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFkey1fJ4_q5"
      },
      "source": [
        "print(\"PREDICTED HEATMAP\")\n",
        "plotting_data(single_cv_img,cv_img_heatmap_pred.numpy(),single_cv_gt_validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfjocjZa55KL"
      },
      "source": [
        "train_images = os.listdir(\"/content/drive/MyDrive/person_train_images\")\n",
        "train_datagen = train_generator(train_images,training_annotations,batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3HCziJ05PpR"
      },
      "source": [
        "train_imgs_batch,train_gt_heatmaps_batch,train_gt_validities_batch = next(train_datagen)\n",
        "single_train_img = train_imgs_batch[0,:,:,:]\n",
        "single_train_img = single_train_img.reshape(1,single_train_img.shape[0],single_train_img.shape[1],\n",
        "                                            single_train_img.shape[2])\n",
        "single_train_gt_heatmap = train_gt_heatmaps_batch[0,:,:,:]\n",
        "single_train_gt_heatmap = single_train_gt_heatmap.reshape(1,single_train_gt_heatmap.shape[0],\n",
        "                                                          single_train_gt_heatmap.shape[1],\n",
        "                                                          single_train_gt_heatmap.shape[2])\n",
        "single_train_gt_validity = train_gt_validities_batch[0,:,:,:]\n",
        "single_train_gt_validity = single_train_gt_validity.reshape(1,single_train_gt_validity.shape[0],\n",
        "                                                            single_train_gt_validity.shape[1],\n",
        "                                                            single_train_gt_validity.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlk-tLHy9h3x"
      },
      "source": [
        "print(\"GROUNDTRUTH HEATMAP\")\n",
        "plotting_data(single_train_img,single_train_gt_heatmap,single_train_gt_validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl1yj7CS_KP6"
      },
      "source": [
        "pretrained_pose_resnet.trainable = False\n",
        "train_img_heatmap_pred = pretrained_pose_resnet(single_train_img,training=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmxVbLTE-fk5"
      },
      "source": [
        "print(\"PREDICTED HEATMAP\")\n",
        "plotting_data(single_train_img,train_img_heatmap_pred.numpy(),single_train_gt_validity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1o9sqBn_pXg"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}